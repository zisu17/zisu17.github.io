---
title: "[Spark] 스파크 스트리밍을 활용한 준실시간 데이터처리"
excerpt: "카프카와 스파크 스트리밍을 활용하여 준실시간 데이터 파이프라인 구축하기"

categories:
  - Data
tags:
  - [Data]

permalink: /data/[Spark]-스파크-스트리밍을-활용한-준실시간-데이터처리/

toc: true
toc_sticky: true

date: 2024-05-24
last_modified_at: 2024-05-24
---

# 아파치 스파크(Apache Spark)의 세션 관리형 도구
아파치 스파크는 데이터 처리를 위해 설계된 오픈 소스 분산형 컴퓨팅 시스템입니다. 
DAG 기반의 작업 최적화 엔진을 사용해 각 작업을 어떻게 최적화할지 결정하고 인메모리 데이터 처리와 캐싱 방식으로 데이터를 빠르게 처리하는 것이 가장 큰 강점입니다. 
오늘은 스파크의 기능을 확장하고 보완하는 써드파티 기술들을 소개하고 비교 분석해보고자 합니다.

### 주요 기능 확인
* 애플리케이션 통합 용이성
* 원격 작업 및 세션 관리 가능 여부
* 멀티 테넌시 지원 여부

## 아파치 리비(Apache Livy)
아파치 리비는 REST API를 통해 스파크 작업을 원격으로 제출하고 관리할 수 있도록하는 서비스입니다. 
리비는 스파크 클러스터 자원을 관리하고 여러 애플리케이션에 쉽게 통합하기 위해서 클라우데라의 프로젝트로 시작되었고 현재는 아파치 인큐베이터 프로젝트로 발전하고 있습니다. 
스파크 클러스터와 애플리케이션 서버 간의 인터랙티브한 환경을 구축할 수 있어 대화형 웹, 앱 애플리케이션에 쉽게 스파크를 통합할 수 있습니다. 

### 주요 특징
![img_4.png](/assets/images/2024-05-12-%5BSpark%5D-스파크-어플리케이션-실행-단계-이해하기/img_4.png)
* 스파크 클러스터에 설치하여 REST API를 통해 스파크 작업 제출 및 관리 가능
* 여러 클라이언트에 독립적인 세션을 제공하여 멀티 테넌시 지원
* 단일 스파크 작업 뿐만 아니라 대화형 세션을 통해 여러 작업을 한 번에 실행 가능
* 비동기 스파크 작업 지원
* Python, Scala, R 등 다중 언어 지원

### 사용 방법
* 세션 생성  

```python
data = {'kind': 'pyspark'}
r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers)
r.json()
```

* 스파크 코드 제출  

```python
data = {
  'code': textwrap.dedent("""
    import random
    NUM_SAMPLES = 100000
    def sample(p):
      x, y = random.random(), random.random()
      return 1 if x*x + y*y < 1 else 0

    count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b)
    print "Pi is roughly %f" % (4.0 * count / NUM_SAMPLES)
    """)
}

r = requests.post(statements_url, data=json.dumps(data), headers=headers)
pprint.pprint(r.json())
```

* Livy Web UI를 통해 작업 상태 및 로그 모니터링  
![img_5.png](/assets/images/2024-05-12-%5BSpark%5D-스파크-어플리케이션-실행-단계-이해하기/img_5.png)


## 스파크 매직(Sparkmagic)
스파크 매직은 주피터 노트북 환경에서 스파크 클러스터를 원활하게 사용할 수 있도록 지원하는 도구입니다. 
클라이언트 서버는 복잡한 설정 없이 스파크를 활용한 대화형 데이터 분석이나 가공이 가능합니다.

### 주요 특징
![스파크매직2](/assets/images/2024-05-12-%5BSpark%5D-스파크-어플리케이션-실행-단계-이해하기/img2.png)
* 위와 같은 아키텍처로 주피터 서버에 스파크 구성요소를 설치할 필요 없이 원격으로 스파크 코드 실행 가능
* Python, Scala, R 커널 등 다중 언어 지원
* 하나의 노트북 서버에서 여러 원격 스파크 클러스터에 접근 가능
* Livy를 사용하기 때문에 동시에 여러 세션을 관리하는 멀티 테넌시 지원
* Pandas나 Plotly 같은 Python 라이브러리와 쉽게 통합 가능하기 때문에 데이터 분석, 시각화에 용이
* 여러 매직 커맨드를 사용하여 SQL 쿼리를 실행하거나 스파크 config 수정 가능

### 사용 방법
* 스파크 매직 실행  
![스파크매직1](/assets/images/2024-05-12-%5BSpark%5D-스파크-어플리케이션-실행-단계-이해하기/img.png)

* 확장 프로그램 로드 및 세션 등록  
![img.png](/assets/images/2024-05-12-%5BSpark%5D-스파크-어플리케이션-실행-단계-이해하기/img0.png)

* 세션을 사용하여 스파크 코드 실행  
![img_1.png](/assets/images/2024-05-12-%5BSpark%5D-스파크-어플리케이션-실행-단계-이해하기/img_1.png)

* 활용 가능한 매직 커맨드  
![img_3.png](/assets/images/2024-05-12-%5BSpark%5D-스파크-어플리케이션-실행-단계-이해하기/img_3.png)


## 아파치 큐비(Apache Kyuubi)
아파치 큐비는 스파크 기반의 오픈 소스 분산 SQL 엔진으로 원격 환경에서 인터랙티브한 스파크 SQL 쿼리 기능을 지원합니다. 
JDBC/ODBC 인터페이스 통해 데이터를 가공할 수 있도록 SQL 게이트웨이를 제공하며 SQL을 주로 사용하는 애플리케이션 환경에 적합합니다. 

### 주요 특징
![img_6.png](/assets/images/2024-05-12-%5BSpark%5D-스파크-어플리케이션-실행-단계-이해하기/img_6.png)
* 여러 클라이언트에 독립적인 세션을 제공하여 멀티 테넌시 지원
* JDBC와 ODBC를 지원하여 기존 BI 도구나 데이터 분석 툴과 쉽게 통합 가능
* REST API를 통해 세션 리소스 관리 및 스파크 SQL문 제출 가능
* HA 아키텍처 및 로드밸런싱으로 시스템 아키텍처의 가용성 향상 
* 비동기 스파크 작업 지원

### 사용 방법
* REST API 호출을 통해 세션 생성  

```python
import requests

kyuubi_url = "http://localhost:10009/sessions"

# 세션 생성을 위한 설정
session_conf = {
    "conf": {
        "spark.app.name": "MySession",
        "spark.master": "local[2]",
        "spark.executor.memory": "2g",
        "spark.executor.cores": "2",
        "spark.sql.shuffle.partitions": "4"
    }
}

response = requests.post(kyuubi_url, json=session_conf)
```

* 세션에 연결된 데이터베이스에 SQL 쿼리 실행하기  

```python
import requests

kyuubi_url = "http://localhost:10009/sessions/123/operations/statement"
sql_code = "SELECT * FROM table"
response = requests.post(kyuubi_url, json={"code": sql_code})
```

* UI로 세션 및 작업 상태 모니터링  
![img_3.png](/assets/images/2024-05-12-%5BSpark%5D-스파크-어플리케이션-실행-단계-이해하기/큐비4.png)


## 번외 : 스파크 커넥트 (Spark Connect)
![img.png](/assets/images/2024-05-12-%5BSpark%5D-스파크-어플리케이션-실행-단계-이해하기/스파크%20커넥트.png)
스파크 커넥트는 스파크 데이터프레임 API를 활용해서 스파크 클러스터에 대해 원격 연결하는 클라이언트-서버 아키텍처의 컴포넌트입니다. 
기존에 SQL이 아닌 데이터프레임 API를 활용하여 스파크 클러스터에 원격 연결하는 기본 기능은 없었는데 3.4 업데이트를 하며 추가되었습니다. 
스파크 커넥트는 Spark Connect API를 활용하여 클라이언트 시스템과 스파크 클러스터간의 범용적인 프로토콜로 통신이 가능합니다. 


### 주요 특징
* 스파크 클러스터에 원격으로 연결하여 데이터를 전송하거나 스파크 작업 실행 가능
* 세션 관리를 지원하여 멀티테넌시 제공
* Python, Scala, R 등의 언어 제공 (3.5 이상)


### 사용 방법
* 스파크 커넥트와 함께 스파크 서버 실행

```
!$HOME/sbin/start-connect-server.sh --packages org.apache.spark:spark-connect_2.12:$SPARK_VERSION
```

* 클라이언트 서버에서 스파크 커넥트 서버로 연결  

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.remote("sc://localhost:15002").getOrCreate()
```

* 데이터프레임 생성

```python
from datetime import datetime, date
from pyspark.sql import Row

df = spark.createDataFrame([
    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),
    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),
    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))
])
df.show()
```

```
+---+---+-------+----------+-------------------+
|  a|  b|      c|         d|                  e|
+---+---+-------+----------+-------------------+
|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|
|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|
|  4|5.0|string3|2000-03-01|2000-01-03 12:00:00|
+---+---+-------+----------+-------------------+

```
