---
title: "[Spark] 스파크 세션 관리형 도구 소개"
excerpt: "스파크 세션 및 원격 작업 등을 관리할 수 있는 다양한 도구 알아보기"

categories:
  - Data
tags:
  - [Data]

permalink: /data/[Spark]-스파크-세션-관리형-도구-소개/

toc: true
toc_sticky: true

date: 2024-05-12
last_modified_at: 2024-05-13
---

# 아파치 스파크(Apache Spark)의 세션 관리형 도구
아파치 스파크는 데이터 처리를 위해 설계된 오픈 소스 분산형 컴퓨팅 시스템입니다. 
DAG 기반의 작업 최적화 엔진을 사용해 각 작업을 어떻게 최적화할지 결정하고 인메모리 데이터 처리와 캐싱 방식으로 데이터를 빠르게 처리하는 것이 가장 큰 강점입니다. 
오늘은 스파크의 기능을 확장하고 보완하는 써드파티 기술들을 소개하고 비교 분석해보고자 합니다.

### 주요 기능 확인
* 애플리케이션 통합 용이성
* 원격 작업 및 세션 관리 가능 여부
* 멀티 테넌시 지원 여부

## 아파치 리비(Apache Livy)
아파치 리비는 REST API를 통해 스파크 작업을 원격으로 제출하고 관리할 수 있도록하는 서비스입니다. 
리비는 스파크 클러스터 자원을 관리하고 여러 애플리케이션에 쉽게 통합하기 위해서 클라우데라의 프로젝트로 시작되었고 현재는 아파치 인큐베이터 프로젝트로 발전하고 있습니다. 
스파크 클러스터와 애플리케이션 서버 간의 인터랙티브한 환경을 구축할 수 있어 대화형 웹, 앱 애플리케이션에 쉽게 스파크를 통합할 수 있습니다. 


### 주요 특징
![img_4.png](/assets/images/2024-05-12-%5BSpark%5D-스파크-어플리케이션-실행-단계-이해하기/img_4.png)
* 스파크 클러스터에 설치하여 REST API를 통해 스파크 작업 제출 및 관리 가능
* 여러 클라이언트에 독립적인 세션을 제공하여 멀티 테넌시 지원
* 단일 스파크 작업 뿐만 아니라 대화형 세션을 통해 여러 작업을 한 번에 실행 가능
* 비동기 스파크 작업 지원
* Python, Scala, R 등 다중 언어 지원

### 사용 방법
1. 세션 생성
```
data = {'kind': 'pyspark'}
r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers)
r.json()

{u'id': 1, u'state': u'idle'}
```

2. 스파크 코드 제출
```
data = {
  'code': textwrap.dedent("""
    import random
    NUM_SAMPLES = 100000
    def sample(p):
      x, y = random.random(), random.random()
      return 1 if x*x + y*y < 1 else 0

    count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b)
    print "Pi is roughly %f" % (4.0 * count / NUM_SAMPLES)
    """)
}

r = requests.post(statements_url, data=json.dumps(data), headers=headers)
pprint.pprint(r.json())

{u'id': 12,
u'output': {u'data': {u'text/plain': u'Pi is roughly 3.136000'},
            u'execution_count': 12,
            u'status': u'ok'},
u'state': u'running'}
```


3. Livy Web UI를 통해 작업 상태 및 로그 모니터링
![img_5.png](/assets/images/2024-05-12-%5BSpark%5D-스파크-어플리케이션-실행-단계-이해하기/img_5.png)

## 스파크 매직(Sparkmagic)
스파크 매직은 주피터 노트북 환경에서 스파크 클러스터를 원활하게 사용할 수 있도록 지원하는 도구입니다. 
클라이언트 서버는 복잡한 설정 없이 스파크를 활용한 대화형 데이터 분석이나 가공이 가능합니다.

### 주요 특징
![스파크매직2](/assets/images/2024-05-12-%5BSpark%5D-스파크-어플리케이션-실행-단계-이해하기/img2.png)
* 위와 같은 아키텍처로 주피터 서버에 스파크 구성요소를 설치할 필요 없이 원격으로 스파크 코드 실행 가능
* Python, Scala, R 커널 등 다중 언어 지원
* 하나의 노트북 서버에서 여러 원격 스파크 클러스터에 접근 가능
* Livy를 사용하기 때문에 동시에 여러 세션을 관리하는 멀티 테넌시 지원
* Pandas나 Plotly 같은 Python 라이브러리와 쉽게 통합 가능하기 때문에 데이터 분석, 시각화에 용이
* 여러 매직 커맨드를 사용하여 SQL 쿼리를 실행하거나 스파크 config 수정 가능

### 사용 방법
1. 스파크 매직 실행
![스파크매직1](/assets/images/2024-05-12-%5BSpark%5D-스파크-어플리케이션-실행-단계-이해하기/img.png)

2. 확장 프로그램 로드 및 세션 등록
![img.png](/assets/images/2024-05-12-%5BSpark%5D-스파크-어플리케이션-실행-단계-이해하기/img0.png)

3. 세션을 사용하여 스파크 코드 실행
![img_1.png](/assets/images/2024-05-12-%5BSpark%5D-스파크-어플리케이션-실행-단계-이해하기/img_1.png)

4. 활용 가능한 매직 커맨드
![img_3.png](/assets/images/2024-05-12-%5BSpark%5D-스파크-어플리케이션-실행-단계-이해하기/img_3.png)


## 아파치 큐비(Apache Kyuubi)
아파치 큐비는 스파크 기반의 오픈 소스 분산 SQL 엔진으로 원격 환경에서 인터랙티브한 스파크 SQL 쿼리 기능을 지원합니다. 
JDBC/ODBC 인터페이스 통해 데이터를 가공할 수 있도록 SQL 게이트웨이를 제공하며 SQL을 주로 사용하는 애플리케이션 환경에 적합합니다. 

### 주요 특징
![img_6.png](/assets/images/2024-05-12-%5BSpark%5D-스파크-어플리케이션-실행-단계-이해하기/img_6.png)
* 여러 클라이언트에 독립적인 세션을 제공하여 멀티 테넌시 지원
* JDBC와 ODBC를 지원하여 기존 BI 도구나 데이터 분석 툴과 쉽게 통합 가능
* REST API를 통해 세션 리소스 관리 및 스파크 SQL문 제출 가능
* HA 아키텍처 및 로드밸런싱으로 시스템 아키텍처의 가용성 향상 
* 비동기 스파크 작업 지원

### 사용 방법
1. REST API 호출을 통해 세션 생성
```python
import requests

kyuubi_url = "http://localhost:10009/sessions"

# 세션 생성을 위한 설정
session_conf = {
    "conf": {
        "spark.app.name": "MySession",
        "spark.master": "local[2]",
        "spark.executor.memory": "2g",
        "spark.executor.cores": "2",
        "spark.sql.shuffle.partitions": "4"
    }
}

response = requests.post(kyuubi_url, json=session_conf)
```

2. 세션에 연결된 데이터베이스에 SQL 쿼리 실행하기
```python
import requests

kyuubi_url = "http://localhost:10009/sessions/123/operations/statement"
sql_code = "SELECT * FROM table"
response = requests.post(kyuubi_url, json={"code": sql_code})
```

3. UI로 세션 및 작업 상태 모니터링
![img_3.png](/assets/images/2024-05-12-%5BSpark%5D-스파크-어플리케이션-실행-단계-이해하기/큐비4.png)
