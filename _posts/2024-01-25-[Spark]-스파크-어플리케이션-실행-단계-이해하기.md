---
title: "[Spark] 스파크 어플리케이션 실행 단계 이해하기"
excerpt: "스파크 어플리케이션 최적화를 위한 실행 단계 이해"

categories:
  - Data
tags:
  - [Data]

permalink: /data/[Spark]-스파크-어플리케이션-실행-단계-이해하기/

toc: true
toc_sticky: true

date: 2024-01-25
last_modified_at: 2024-01-25
---

# Spark 어플리케이션 실행 단계
스파크 코드를 최적화하려면 어플리케이션이 어떻게 실행되고 어느 부분에서 병목이 일어나는지 잘 파악하는 것이 중요하다. 
스파크 어플리케이션은 데이터의 로드, 처리, 분석 및 저장 등 다양한 작업을 수행하는데 이런 작업을 효율적으로 관리하기 위해 스파크는 어플리케이션을 여러 단계로 나눈다. 
어플리케이션(application)은 잡(Job), 스테이지(Stage), 태스크(Task)로 나뉘게 되는데 이 구조는 스파크가 데이터를 병렬로 빠르게 처리하는데 중요한 역할을 한다. 
이 실행 단계를 알아두면 데이터 셔플링, 태스크 실행 시간, 스테이지 처리 시간 등 성능에 영향을 미치는 요소를 디버깅 하기 수월해진다. 
오늘은 이 스파크 어플리케이션의 실행 단계 `Job 생성 - Stage 분할 - Task 할당 - 실행 및 결과 반환` 이라는 일련의 과정을 자세히 알아보려고 한다. 

## Application 시작
스파크 어플리케이션은 스파크 클러스터 위에서 실행되는 전체 프로그램을 말한다. 
어플리케이션은 스파크 컨텍스트를 통해 시작되는데 스파크 클러스터와의 통신을 관리하고 작업을 스케줄링하는 등의 역할을 한다. 
또한 어플리케이션은 데이터를 로드, 변환, 분석하고 결과를 저장하는 등의 작업을 수행한다. 
이 과정에서 하나 이상의 잡을 생성하고, 각 잡은 다시 여러 스테이지와 태스크로 나눠진다.

## Job 생성
Job은 스파크 어플리케이션에서 실행되는 개별 작업 단위이다. 
어플리케이션은 하나 이상의 Job을 생성하며 일반적으로 스파크 액션에 의해 트리거 된다. 
여기서 액션 연산은 (ex. `collect()`, `count()`, `save()` 등) 실제 데이터를 반환하거나 외부 시스템에 저장하는 연산을 말한다. 
Job은 트랜스포메이션 연산(ex. `map()`, `filter()`, `groupBy()` 등)의 DAG을 기반으로 하고 실제 Job의 생성과 실행은 액션 연산이 호출될 때 시작되기 때문에 사전에 몇 개의 job이 실행될지 명확히 알기 어렵다 .

하나의 스파크 작업(Statement) 내에서 여러 트랜스포메이션 연산이 정의될 수 있지만, 이 연산들은 실제로 데이터에 적용되지 않습니다. 이들은 DAG에 연산의 흐름과 의존성을 정의하는 노드와 엣지로만 표현됩니다. 실제 잡의 생성과 실행은 액션 연산(예: count(), collect(), saveAsTextFile() 등)이 호출될 때 시작됩니다.
. DAG는 데이터 처리의 의존성과 순서를 정의하며, 스파크는 이를 분석하여 실행 계획을 세운다. 각 잡은 하나 이상의 스테이지로 나누어지며, 스테이지는 다시 여러 태스크로 구성된다.
이 Job은 스파크 드라이버를 통해 스파크에 submit 되는데 이 Job을 실행하기 위해 더 작은 단위로 쪼개지는 것이 Stage이다.

## Stage 분할

Apache Spark는 빅데이터 처리를 위한 오픈소스 분산 컴퓨팅 시스템이며, 스파크 어플리케이션은 이 시스템 위에서 실행되는 사용자 프로그램을 의미한다. 


### 잡(Job)

스파크 어플리케이션 내에서, 잡은 액션 연산에 의해 트리거된 독립적인 작업 단위이다. 액션 연산이란, 예를 들어 

### 스테이지(Stage)

스테이지는 잡을 구성하는 중간 단계로, 스파크가 잡의 DAG를 분석한 결과에 기반하여 생성된다. 스테이지는 주로 데이터의 셔플이 필요한 지점에서 분리된다. 셔플이란, 다른 노드로 데이터를 재분배하는 과정을 말하며, 이는 예를 들어 `groupBy()`와 같은 연산에서 발생한다. 각 스테이지는 셔플 연산 이전까지의 모든 트랜스포메이션을 포함하며, 스테이지 내의 태스크들은 병렬로 실행될 수 있다. 스테이지의 실행은 모든 입력 데이터가 준비되었을 때 시작되며, 모든 태스크의 실행이 완료되면 다음 스테이지로 넘어간다.

### 태스크(Task)

태스크는 스파크에서 실행되는 가장 작은 작업 단위이며, 스테이지 내의 개별 연산을 담당한다. 각 태스크는 클러스터의 단일 노드에서 실행되며, 입력 데이터의 한 파티션에 대한 연산을 수행한다. 스파크는 데이터를 파티션으로 나누고, 각 파티션에 대해 태스크를 할당하여 병렬 처리를 수행한다. 태스크는 독립적으로 실행되며, 실패할 경우 스파크에 의해 자동으로 재시도될 수 있다.

스파크 어플리케이션의 실행 단계(어플리케이션, 잡, 스테이지, 태스크)는 데이터 처리의 복잡성을 관리하고, 클러스터의 리소스를 효율적으로 사용하기 위한 핵심 구성요소이다. 이 구조를 통해 스파크는 대규모 데이터셋을 빠르고 효율적으로 처리할 수 있다. 어플리케이션에서의 고수준 추상화는 사용자가 복잡한 분산 처리 메커니즘을 신경 쓰지 않고도 데이터 처리 로직을 구현할 수 있게 해준다. 동시에, 스파크의 실행 모델은 자동 스케일링, 복구, 최적화 등의 기능을 제공하여 빅데이터 애플리케이션의 개발과 운영을 단순화한다.
