---
title: "[Spark] 스파크 어플리케이션 실행 단계 이해하기"
excerpt: "스파크 어플리케이션 최적화를 위한 실행 단계 이해"

categories:
  - Data
tags:
  - [Data]

permalink: /data/[Spark]-스파크-어플리케이션-실행-단계-이해하기/

toc: true
toc_sticky: true

date: 2024-01-25
last_modified_at: 2024-01-25
---

# Spark 어플리케이션 실행 단계
스파크 코드를 최적화하려면 어플리케이션이 어떻게 실행되고 어느 부분에서 지연이 되는지 잘 파악하는 것이 중요하다.
일반적으로 스파크 어플리케이션을 실행하면 `Job 생성 - Stage 분할 - Task 할당 - 실행 및 결과 반환` 이라는 일련의 과정을 거치게 되는데 
오늘은 이 스파크 어플리케이션의 실행 단계에 대해 알아보려고 한다. 

## 어플리케이션 시작
스파크 어플리케이션을 실행하려면 맨 처음 SparkContext 객체를 생성해야 한다. 
이 SparkContext는 스파크 어플리케이션의 진입점이면서 핵심 요소가 되는데 스파크 클러스터와의 커넥션 관리, RDD 생성과 변환, 작업 스케줄링 등의 역할을 맡는다. 

## Job 생성
Job은 스파크 어플리케이션에서 실행되는 개별 작업 단위이다. 어플리케이션은 하나 이상의 Job을 생성하며 일반적으로 스파크 액션(count, collect, first 등)에 의해 트리거 된다. 
이 Job은 스파크 드라이버를 통해 스파크에 submit 되는데 이 Job을 실행하기 위해 더 작은 단위로 쪼개지는 것이 Stage이다.

## Stage 